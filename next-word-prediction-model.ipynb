{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12062841,"sourceType":"datasetVersion","datasetId":7592671}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Next Word Prediction means predicting the most likely word or phrase that will come next in a sentence or text. It is like having an inbuilt feature on an application that suggests the next word as you type or speak. The Next Word Prediction Models are used in applications like messaging apps, search engines, virtual assistants, and autocorrect features on smartphones. So, if you want to learn how to build a Next Word Prediction Model, this article is for you. In this article, I’ll take you through building a Next Word Prediction Model with Deep Learning using Python.","metadata":{}},{"cell_type":"markdown","source":"Next word prediction is a language modelling task in Machine Learning that aims to predict the most \nprobable word or sequence of words that follows a given input context. \n\nThis task utilizes statistical patterns and linguistic structures to generate accurate predictions\nbased on the context provided.","metadata":{}},{"cell_type":"markdown","source":"The Next Word Prediction models have a range of applications across various industries. For example, when you start typing a message on your phone, it suggests the next word to speed up your typing. Similarly, search engines predict and show search suggestions as you type in the search bar. Next word prediction helps us communicate faster and more accurately by anticipating what we might say or search for.\n\nTo build a Next Word Prediction model:\n\nstart by collecting a diverse dataset of text documents, \npreprocess the data by cleaning and tokenizing it, \nprepare the data by creating input-output pairs, \nengineer features such as word embeddings, \nselect an appropriate model like an LSTM or GPT, \ntrain the model on the dataset while adjusting hyperparameters,\nimprove the model by experimenting with different techniques and architectures.\nThis iterative process allows businesses to develop accurate and efficient Next Word Prediction models that can be applied in various applications.\n\nSo the process of building a Next Word Prediction model starts by collecting textual data that can be a vocabulary for our model. For example, the way you type on your smartphone’s keyboard becomes the vocabulary of the next word prediction model of your smartphone’s keyboard. In the same way, we need textual data for our model. I found an ideal dataset for this task based on the text of a book on Sherlock Holmes. You can download the dataset from here.\n\nNext Word Prediction Model using Python\nI hope you now know what a Next Word Prediction model is. In this section, I’ll take you through how to build a Next Word Prediction model using Python and Deep Learning. So, let’s start this task by importing the necessary Python libraries and the dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:08.822792Z","iopub.execute_input":"2025-06-10T20:25:08.823011Z","iopub.status.idle":"2025-06-10T20:25:30.510212Z","shell.execute_reply.started":"2025-06-10T20:25:08.822995Z","shell.execute_reply":"2025-06-10T20:25:30.509428Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"2025-06-10 20:25:12.400749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749587112.889524      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749587113.017095      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"with open('/kaggle/input/book-data/sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n    text = file.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:30.511057Z","iopub.execute_input":"2025-06-10T20:25:30.511607Z","iopub.status.idle":"2025-06-10T20:25:30.564441Z","shell.execute_reply.started":"2025-06-10T20:25:30.511582Z","shell.execute_reply":"2025-06-10T20:25:30.563747Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#tokenizing the text to create a sequence of words\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([text])\ntotal_words = len(tokenizer.word_index) + 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:30.565998Z","iopub.execute_input":"2025-06-10T20:25:30.566202Z","iopub.status.idle":"2025-06-10T20:25:30.672273Z","shell.execute_reply.started":"2025-06-10T20:25:30.566186Z","shell.execute_reply":"2025-06-10T20:25:30.671726Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"In the above code, the text is tokenized, which means it is divided into individual words or tokens. The ‘Tokenizer’ object is created, which will handle the tokenization process. The ‘fit_on_texts’ method of the tokenizer is called, passing the ‘text’ as input. This method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index. The ‘total_words’ variable is then assigned the value of the length of the word index plus one, representing the total number of distinct words in the text.","metadata":{}},{"cell_type":"markdown","source":"Now let’s create input-output pairs by splitting the text into sequences of tokens and forming n-grams from the sequences:","metadata":{}},{"cell_type":"code","source":"input_sequences = []\nfor line in text.split('\\n'):\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequences = token_list[:i+1]\n        input_sequences.append(n_gram_sequences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:30.672965Z","iopub.execute_input":"2025-06-10T20:25:30.673169Z","iopub.status.idle":"2025-06-10T20:25:30.809932Z","shell.execute_reply.started":"2025-06-10T20:25:30.673153Z","shell.execute_reply":"2025-06-10T20:25:30.809435Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"input_sequences[0:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:30.810522Z","iopub.execute_input":"2025-06-10T20:25:30.810692Z","iopub.status.idle":"2025-06-10T20:25:30.816165Z","shell.execute_reply.started":"2025-06-10T20:25:30.810678Z","shell.execute_reply":"2025-06-10T20:25:30.815586Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[[1, 1561],\n [1, 1561, 5],\n [1, 1561, 5, 129],\n [1, 1561, 5, 129, 34],\n [647, 4498],\n [647, 4498, 4499],\n [226, 5],\n [226, 5, 1562],\n [6, 827],\n [6, 827, 7]]"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"In the above code, the text data is split into lines using the ‘\\n’ character as a delimiter. For each line in the text, the ‘texts_to_sequences’ method of the tokenizer is used to convert the line into a sequence of numerical tokens based on the previously created vocabulary. The resulting token list is then iterated over using a for loop. For each iteration, a subsequence, or n-gram, of tokens is extracted, ranging from the beginning of the token list up to the current index ‘i’.\n\nThis n-gram sequence represents the input context, with the last token being the target or predicted word. This n-gram sequence is then appended to the ‘input_sequences’ list. This process is repeated for all lines in the text, generating multiple input-output sequences that will be used for training the next word prediction model.\n\nNow let’s pad the input sequences to have equal length:","metadata":{}},{"cell_type":"code","source":"max_sequence_len = max([len(seq) for seq in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding = 'pre'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:30.816869Z","iopub.execute_input":"2025-06-10T20:25:30.817049Z","iopub.status.idle":"2025-06-10T20:25:31.027312Z","shell.execute_reply.started":"2025-06-10T20:25:30.817027Z","shell.execute_reply":"2025-06-10T20:25:31.026745Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"input_sequences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:31.027988Z","iopub.execute_input":"2025-06-10T20:25:31.028191Z","iopub.status.idle":"2025-06-10T20:25:31.033499Z","shell.execute_reply.started":"2025-06-10T20:25:31.028175Z","shell.execute_reply":"2025-06-10T20:25:31.032907Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([[   0,    0,    0, ...,    0,    1, 1561],\n       [   0,    0,    0, ...,    1, 1561,    5],\n       [   0,    0,    0, ..., 1561,    5,  129],\n       ...,\n       [   0,    0,    0, ...,    1, 8198, 8199],\n       [   0,    0,    0, ..., 8198, 8199, 3187],\n       [   0,    0,    0, ..., 8199, 3187, 3186]], dtype=int32)"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"In the above code, the input sequences are padded to ensure all sequences have the same length. The variable ‘max_sequence_len’ is assigned the maximum length among all the input sequences. The ‘pad_sequences’ function is used to pad or truncate the input sequences to match this maximum length.\n\nThe ‘pad_sequences’ function takes the input_sequences list, sets the maximum length to ‘max_sequence_len’, and specifies that the padding should be added at the beginning of each sequence using the ‘padding=pre’ argument. Finally, the input sequences are converted into a numpy array to facilitate further processing.\n\nNow let’s split the sequences into input and output:","metadata":{}},{"cell_type":"code","source":"X= input_sequences[:,:-1]\ny = input_sequences[:,-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:31.034195Z","iopub.execute_input":"2025-06-10T20:25:31.034482Z","iopub.status.idle":"2025-06-10T20:25:31.051563Z","shell.execute_reply.started":"2025-06-10T20:25:31.034459Z","shell.execute_reply":"2025-06-10T20:25:31.050997Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"In the above code, the input sequences are split into two arrays, ‘X’ and ‘y’, to create the input and output for training the next word prediction model. The ‘X’ array is assigned the values of all rows in the ‘input_sequences’ array except for the last column. It means that ‘X’ contains all the tokens in each sequence except for the last one, representing the input context.\n\nOn the other hand, the ‘y’ array is assigned the values of the last column in the ‘input_sequences’ array, which represents the target or predicted word.\n\nNow let’s convert the output to one-hot encode vectors:","metadata":{}},{"cell_type":"code","source":"y = np.array(tf.keras.utils.to_categorical(y,num_classes = total_words))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:31.053850Z","iopub.execute_input":"2025-06-10T20:25:31.054200Z","iopub.status.idle":"2025-06-10T20:25:35.310576Z","shell.execute_reply.started":"2025-06-10T20:25:31.054185Z","shell.execute_reply":"2025-06-10T20:25:35.309974Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"total_words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:35.311224Z","iopub.execute_input":"2025-06-10T20:25:35.311432Z","iopub.status.idle":"2025-06-10T20:25:35.315931Z","shell.execute_reply.started":"2025-06-10T20:25:35.311418Z","shell.execute_reply":"2025-06-10T20:25:35.315234Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"8200"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"In the above code, we are converting the output array into a suitable format for training a model, where each target word is represented as a binary vector.\n\nNow let’s build a neural network architecture to train the model:","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(total_words,100, input_length=max_sequence_len - 1))\nmodel.add(LSTM(150))\nmodel.add(Dense(total_words,activation = 'softmax'))\n# Explicitly build the model (optional but ensures the model is built)\nmodel.build(input_shape=(None, max_sequence_len - 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:35.316755Z","iopub.execute_input":"2025-06-10T20:25:35.317307Z","iopub.status.idle":"2025-06-10T20:25:40.291363Z","shell.execute_reply.started":"2025-06-10T20:25:35.317279Z","shell.execute_reply":"2025-06-10T20:25:40.290583Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1749587138.215794      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1749587138.216484      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:40.292213Z","iopub.execute_input":"2025-06-10T20:25:40.292804Z","iopub.status.idle":"2025-06-10T20:25:40.307876Z","shell.execute_reply.started":"2025-06-10T20:25:40.292776Z","shell.execute_reply":"2025-06-10T20:25:40.307288Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │         \u001b[38;5;34m820,000\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)                 │         \u001b[38;5;34m150,600\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8200\u001b[0m)                │       \u001b[38;5;34m1,238,200\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">820,000</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8200</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,238,200</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,208,800\u001b[0m (8.43 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,208,800</span> (8.43 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,208,800\u001b[0m (8.43 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,208,800</span> (8.43 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"The code above defines the model architecture for the next word prediction model. The ‘Sequential’ model is created, which represents a linear stack of layers. The first layer added to the model is the ‘Embedding’ layer, which is responsible for converting the input sequences into dense vectors of fixed size. It takes three arguments:\n\n‘total_words’, which represents the total number of distinct words in the vocabulary; \n‘100’, which denotes the dimensionality of the word embeddings; \nand ‘input_length’, which specifies the length of the input sequences.\nThe next layer added is the ‘LSTM’ layer, a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data. It has 150 units, which means it will learn 150 internal representations or memory cells.\n\nFinally, the ‘Dense’ layer is added, which is a fully connected layer that produces the output predictions. It has ‘total_words’ units and uses the ‘softmax’ activation function to convert the predicted scores into probabilities, indicating the likelihood of each word being the next one in the sequence.\n\nNow let’s compile and train the model:","metadata":{}},{"cell_type":"code","source":"model.compile(loss = 'categorical_crossentropy', optimizer = 'adam',metrics = ['accuracy'])\nmodel.fit(X,y, epochs = 100,verbose= 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:25:40.308640Z","iopub.execute_input":"2025-06-10T20:25:40.308881Z","iopub.status.idle":"2025-06-10T20:53:46.297596Z","shell.execute_reply.started":"2025-06-10T20:25:40.308856Z","shell.execute_reply":"2025-06-10T20:53:46.296949Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1749587152.440705      94 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - accuracy: 0.0613 - loss: 6.5721\nEpoch 2/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.1128 - loss: 5.6158\nEpoch 3/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.1417 - loss: 5.1906\nEpoch 4/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.1598 - loss: 4.8527\nEpoch 5/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.1757 - loss: 4.5559\nEpoch 6/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.1971 - loss: 4.2616\nEpoch 7/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.2234 - loss: 3.9785\nEpoch 8/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.2491 - loss: 3.7139\nEpoch 9/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.2841 - loss: 3.4596\nEpoch 10/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.3181 - loss: 3.2370\nEpoch 11/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.3559 - loss: 3.0170\nEpoch 12/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.3856 - loss: 2.8464\nEpoch 13/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.4234 - loss: 2.6493\nEpoch 14/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.4506 - loss: 2.5016\nEpoch 15/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.4781 - loss: 2.3597\nEpoch 16/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.5080 - loss: 2.2103\nEpoch 17/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.5373 - loss: 2.0838\nEpoch 18/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.5633 - loss: 1.9638\nEpoch 19/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.5815 - loss: 1.8771\nEpoch 20/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.6050 - loss: 1.7624\nEpoch 21/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.6249 - loss: 1.6763\nEpoch 22/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.6454 - loss: 1.5763\nEpoch 23/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.6584 - loss: 1.5161\nEpoch 24/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.6764 - loss: 1.4315\nEpoch 25/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.6907 - loss: 1.3666\nEpoch 26/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.7048 - loss: 1.3083\nEpoch 27/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.7196 - loss: 1.2515\nEpoch 28/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.7279 - loss: 1.1983\nEpoch 29/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.7398 - loss: 1.1483\nEpoch 30/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.7489 - loss: 1.1141\nEpoch 31/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.7583 - loss: 1.0649\nEpoch 32/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.7671 - loss: 1.0240\nEpoch 33/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.7774 - loss: 0.9825\nEpoch 34/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.7840 - loss: 0.9524\nEpoch 35/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.7903 - loss: 0.9206\nEpoch 36/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.7970 - loss: 0.8955\nEpoch 37/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8024 - loss: 0.8709\nEpoch 38/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8067 - loss: 0.8452\nEpoch 39/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8112 - loss: 0.8263\nEpoch 40/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8165 - loss: 0.8039\nEpoch 41/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8185 - loss: 0.7810\nEpoch 42/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8246 - loss: 0.7641\nEpoch 43/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8290 - loss: 0.7412\nEpoch 44/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8322 - loss: 0.7236\nEpoch 45/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8371 - loss: 0.7030\nEpoch 46/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.8360 - loss: 0.7066\nEpoch 47/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8411 - loss: 0.6874\nEpoch 48/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8421 - loss: 0.6746\nEpoch 49/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8460 - loss: 0.6594\nEpoch 50/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.8482 - loss: 0.6477\nEpoch 51/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8487 - loss: 0.6463\nEpoch 52/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8498 - loss: 0.6288\nEpoch 53/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8523 - loss: 0.6221\nEpoch 54/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8548 - loss: 0.6110\nEpoch 55/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8576 - loss: 0.5988\nEpoch 56/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8562 - loss: 0.6032\nEpoch 57/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8592 - loss: 0.5900\nEpoch 58/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8599 - loss: 0.5821\nEpoch 59/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8623 - loss: 0.5702\nEpoch 60/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8624 - loss: 0.5685\nEpoch 61/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8644 - loss: 0.5606\nEpoch 62/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8635 - loss: 0.5601\nEpoch 63/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8693 - loss: 0.5437\nEpoch 64/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8646 - loss: 0.5493\nEpoch 65/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.8698 - loss: 0.5319\nEpoch 66/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8678 - loss: 0.5405\nEpoch 67/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8686 - loss: 0.5348\nEpoch 68/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8699 - loss: 0.5315\nEpoch 69/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8664 - loss: 0.5400\nEpoch 70/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8720 - loss: 0.5236\nEpoch 71/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8712 - loss: 0.5224\nEpoch 72/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8711 - loss: 0.5212\nEpoch 73/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8698 - loss: 0.5192\nEpoch 74/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8736 - loss: 0.5087\nEpoch 75/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8729 - loss: 0.5078\nEpoch 76/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8758 - loss: 0.4962\nEpoch 77/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8755 - loss: 0.4983\nEpoch 78/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8729 - loss: 0.5045\nEpoch 79/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8735 - loss: 0.5012\nEpoch 80/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8716 - loss: 0.5064\nEpoch 81/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8769 - loss: 0.4929\nEpoch 82/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8781 - loss: 0.4920\nEpoch 83/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8742 - loss: 0.4913\nEpoch 84/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.8734 - loss: 0.4983\nEpoch 85/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8748 - loss: 0.4901\nEpoch 86/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8748 - loss: 0.4908\nEpoch 87/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8748 - loss: 0.4880\nEpoch 88/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8753 - loss: 0.4853\nEpoch 89/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8772 - loss: 0.4795\nEpoch 90/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8750 - loss: 0.4840\nEpoch 91/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8758 - loss: 0.4807\nEpoch 92/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8769 - loss: 0.4764\nEpoch 93/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8770 - loss: 0.4817\nEpoch 94/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8814 - loss: 0.4655\nEpoch 95/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8770 - loss: 0.4714\nEpoch 96/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8766 - loss: 0.4767\nEpoch 97/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8792 - loss: 0.4727\nEpoch 98/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8799 - loss: 0.4674\nEpoch 99/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.8782 - loss: 0.4692\nEpoch 100/100\n\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.8795 - loss: 0.4621\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7b99933accd0>"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"In the above code, the model is being compiled and trained. The ‘compile’ method configures the model for training. The ‘loss’ parameter is set to ‘categorical_crossentropy’, a commonly used loss function for multi-class classification problems. The ‘optimizer’ parameter is set to ‘adam’, an optimization algorithm that adapts the learning rate during training.\n\nThe ‘metrics’ parameter is set to ‘accuracy’ to monitor the accuracy during training. After compiling the model, the ‘fit’ method is called to train the model on the input sequences ‘X’ and the corresponding output ‘y’. The ‘epochs’ parameter specifies the number of times the training process will iterate over the entire dataset. The ‘verbose’ parameter is set to ‘1’ to display the training process.\n\nThe above code will take more than an hour to execute. Once the code is executed, here’s how we can generate the next word predictions using our model:","metadata":{}},{"cell_type":"code","source":"seed_text = \"My name is\"\nnext_words = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T21:03:53.879895Z","iopub.execute_input":"2025-06-10T21:03:53.880505Z","iopub.status.idle":"2025-06-10T21:03:53.883716Z","shell.execute_reply.started":"2025-06-10T21:03:53.880479Z","shell.execute_reply":"2025-06-10T21:03:53.883024Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"for _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list],maxlen = max_sequence_len - 1, padding = 'pre')\n    predicted = np.argmax(model.predict(token_list), axis = -1)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\n\nprint(seed_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T21:03:55.948079Z","iopub.execute_input":"2025-06-10T21:03:55.948368Z","iopub.status.idle":"2025-06-10T21:03:56.325800Z","shell.execute_reply.started":"2025-06-10T21:03:55.948348Z","shell.execute_reply":"2025-06-10T21:03:56.325030Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\nMy name is sherlock holmes it is more\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"The above code generates the next word predictions based on a given seed text. The ‘seed_text’ variable holds the initial text. The ‘next_words’ variable determines the number of predictions to be generated. Inside the for loop, the ‘seed_text’ is converted into a sequence of tokens using the tokenizer. The token sequence is padded to match the maximum sequence length.\n\nThe model predicts the next word by calling the ‘predict’ method on the model with the padded token sequence. The predicted word is obtained by finding the word with the highest probability score using ‘np.argmax’. Then, the predicted word is appended to the ‘seed_text’, and the process is repeated for the desired number of ‘next_words’. Finally, the ‘seed_text’ is printed, which contains the initial text followed by the generated predictions.","metadata":{}},{"cell_type":"markdown","source":"Next word prediction is a language modelling task in Machine Learning that aims to predict the most probable word or sequence of words that follows a given input context. This task utilizes statistical patterns and linguistic structures to generate accurate predictions based on the context provided. ","metadata":{}}]}